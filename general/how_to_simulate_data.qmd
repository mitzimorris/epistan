---
title: How to Write a Stan Model to Simulate Data and Why You Should
author: Mitzi Morris
format:
  html:
    code-fold: true
jupyter: python3
---

```{python}
#| echo: false
import os
import numpy as np
import pandas as pd
import json
from random import randint
from cmdstanpy import CmdStanModel, write_stan_json
# notebook display options
np.set_printoptions(precision=2)
np.set_printoptions(suppress=True)
pd.set_option('display.precision', 2)
pd.options.display.float_format = '{:.2f}'.format
```
```{r}
#| echo: false
# Preliminary setup
install.packages('versions')
library(versions)
# Install package CmdStanR from GitHub
library(devtools)
if(!require(cmdstanr)){
  devtools::install_github("stan-dev/cmdstanr", dependencies=c("Depends", "Imports"))
  library(cmdstanr)
}
options(digits = 2)
print_file <- function(file, nlines=-1L) {
  cat(paste(readLines(file, n=nlines), "\n", sep=""), sep="")
}
```

# How to Write a Stan Model to Simulate Data and Why You Should

## Simulated Data 

By "simulated data" we mean data generated from a set of known parameter values.

## How

In the Bayesian setting, we first generate parameters according to known priors and then use these to generate a dataset of the desired size and shape.
To do this in Stan, we use Stan's [RNG functions for probability distributions](https://mc-stan.org/docs/functions-reference/conventions_for_probability_functions.html#distributions-prng)
which may only be used in the transformed data or generated quantities blocks.
In the transformed data block we use them to set up known parameters according to specified priors;
in the generated quantities block we then use the model parameters to generate a dataset according to the some likelihood.
Because we use Stan purely for computation, not inference, we call this Stan code "programs", rather than "models".

## Why

The data-generating program is the companion to the data-fitting model
which infers the model parameters given data, i.e., it "solves the inverse problem".
Using a Stan program instead of data-generating script in Julia, Python, or R
ensures that the same parameterization and implementation the probability distributions
are used to generate the data and fit the model, thus a correctly
implemented model should be able to recover the parameters.
If the model cannot recover the parameters from the generated dataset, then either
the implementation of the data-generating program does not match the model, or the model is fundamentally
misspecified.


# Best Practice:  Start Simple

Starting from a trivial model makes it easy to verify that all components
of the system are functioning properly.
The CmdStan distribution provides a
["Hello, World" model](https://mc-stan.org/docs/cmdstan-guide/example_model_data.html) `bernoulli.stan`,
and corresponding dataset: `bernoulli.data.json`

```stan
data {
  int<lower=0> N;
  array[N] int<lower=0,upper=1> y;
}
parameters {
  real<lower=0,upper=1> theta;
}
model {
  theta ~ beta(1,1);  // uniform prior on interval 0,1
  y ~ bernoulli(theta);  // likelihood
}
```

This model could be used to estimate disease prevalence in the general population given a set of diagnostic test results from a small fraction of the population, under the following assumptions:

 - the test is perfectly accurate
 - the population is completely homogenous
 - the disease prevalence in the general population could be anything, from none to all.

Before we refine this model, we start by building the corresponding data-generating program to test it.
By writing a general enough program, we can use Stan's MCMC sampler to produce multiple datasets,
in order to demonstrate that the data-fitting model can fit any dataset which conforms to the
data-generating process.

### Stan program to generate a dataset (or several)

The Stan program `bernoulli_datagen.stan` generates the observed data `y` according to
a specified parameter `theta`, the probability of success.
The number of observations `N` must be provided as data,
(alternatively is can be stipulated in the `transformed data` block).

+ `theta` is generated in the `transformed data` block by calling the Stan RNG function corresponding to the prior.
+ `y` is generated in the `generated quantities` block according by calling the Stan RNG function corresponding to the likelihood.

```stan
data {
  int N_obs;
}
generated quantities {
  int N = N_obs;
  real theta = uniform_rng(0.0, 1.0);  // probability of success
  array[N] int test_results;  // observed outcomes
  for (n in 1:N) {
    test_results[n] = bernoulli_rng(theta);
  }
}
```

### Running the Data-generating Program

The data-generating program is compiled like any other Stan model.
Since it has no parameters, transformed parameters, or model blocks, there's no need to run any warmup iterations.
Every iteration of this model generates a complete test dataset.
Here we run it for 100 iterations which generates 100 datasets.

```{python}
hello_datagen = CmdStanModel(stan_file='bernoulli_datagen.stan')
hello_data = hello_datagen.sample(data={"N_obs":100}, chains=1, iter_warmup=0, iter_sampling=100, show_progress=False,
```


In CmdStanPy the "." operator + variable name is equivalent to the "stan_variable" function - this allows us to easily inspect the simulated data.

```{python}
print(f'N {hello_data.test_results[0].shape[0]}\n'
      f'theta {hello_data.theta[0]:.2f}\n'
      f'y {hello_data.test_results[0]}')
```

```{python}
for i in range(10):
    print(f'generated param: {hello_data.theta[i]:.2f} '
          f'generated dataset mean: {np.sum(hello_data.test_results[i])/100}')
```

Given 100 bernoulli trials, the resulting dataset averages are not very accurate.  
(Exercise: rerun with N=10000).

In CmdStanR it is necessary to use methods from the `posterior` package to retrieve the generated data in the correct format.


### From Stan Outputs to Stan Inputs

The input to `bernoulli.stan` consists of an integer `N` and an int array of observed results.
To create the simulated dataset, we can use any draw in the sample.
Each draw is generated according to a different value of `theta`; we save this as `theta_true`;


```{python}
theta_true = hello_data.theta[0]
sim_data = {"N" : hello_data.test_results[0].shape[0], "y" : hello_data.test_results[0].astype(int)}
sim_data
```

To save this dataset as a JSON file, we need to use the CmdStanPy function
[write_stan_json](https://mc-stan.org/cmdstanpy/api.html#write-stan-json).

```{python}
write_stan_json("datagen_bernoulli.json", sim_data)
```

In CmdStanR ....


### Fitting the Simulated Data to the Target Model

We have copied over the bernoulli model to this notebook directory.

```{python}
bernoulli_mod = CmdStanModel(stan_file='bernoulli.stan')
bernoulli_fit = bernoulli.sample(data=sim_data)
```

```{python}
bernoulli_fit.summary()
```

```{python}
print(f'data generating param theta {theta_true}')
```

### Efficiency Tuning: From Bernoulli to Binomial

The data is a series of individual Bernoulli trials; this is more efficiently coded using the binomial distribution.

```stan
data {
  int<lower=0> N;
  array[N] int<lower=0,upper=1> y;
}
transformed data {
  int ones = sum(y);
  print(ones);
}
parameters {
  real<lower=0,upper=1> theta;
}
model {
  theta ~ beta(1, 1);
  ones ~ binomial(N, theta);
}
```

See the
[Efficiency Tuning](https://mc-stan.org/docs/stan-users-guide/efficiency-tuning.html#exploiting-sufficient-statistics)
section of the Stan User's Guide for details.

```{python}
binom_prevalence = CmdStanModel(stan_file='m1_binom.stan')
binom_fit = binom_prevalence.sample(data=sim_data)
binom_fit.summary()
```

### Hierarchical Binomial Models of Population Demographics

To account for different disease prevalence across demographic subpopulations,
we naturally use a linear model or GLM to account for the combination of
interactions between cross-cutting factors.

A multi-level model allows for per-category effects.   (See Gelman and Hill chapters 11, 12)
Given a set of categorical predictors used to stratify the population into distinct demographic subgroups,
we can build hierarchical models with a group-level predictor on each category or combination of categories.
For example, given predictors for sex, a fixed number of age ranges,
and a fixed number of race/ethnicity categories, we have the following hierarchical regression:

```stan
data {
  int<lower=1> N; // number of strata
  int<lower=3> N_age, N_eth;
  vector<lower=0, upper=1>[N] sex; // 0 = male, 1 = female
  array[N] int<lower=1, upper=N_age> age;
  array[N] int<lower=1, upper=N_eth> eth;
  array[N] int<lower=0> tests;
  array[N] int<lower=0> pos_tests; // observed outcome
}
parameters {
  real alpha, beta_female;
  real<lower=0> sigma_age, sigma_eth;
  vector<multiplier=sigma_age>[N_age] beta_age;
  vector<multiplier=sigma_eth>[N_eth] beta_eth;
}
model {
  num_pos ~ binomial(tests,
                     alpha + beta_female * sex + beta_age[age] + beta_eth[eth]);
  // priors
  alpha ~ normal(0, 5);
  beta_female ~ normal(0, 2.5);
  beta_age ~ normal(0, sigma_age);
  beta_eth ~ normal(0, sigma_eth);
  sigma_eth ~ normal(0, 2.5);
  sigma_age ~ normal(0, 2.5);
}                     
```

The number of strata `N` is implicitly `2 * N_age * N_eth`,
but using this expression everywhere instead of `N` reduces the program's readability and maintainability.
Furthermore, if the binomial data is aggregated from individual test data,
and if there are no observations available for some subpopulation,
then the size of the aggregated dataset will be less than the product of the category sizes,
and therefore we need to specify `N` as well.

### Generating Data for a Hierarchical Binomial Model

The inputs to the above model are:

- the data dimensions:  `N`, `N_age`, `N_eth` - the number of observations, age categories, and race/ethnicity categories, respectively.
- the per-category binomial outcome `pos_tests`, the number of tests per demographic, and the outcome predictors `sex`, `age`, `eth`.

We can stipulate (or allow the user to stipulate) the data dimensions.
As discussed above, `N` is implicitly the product of the size of all categorical predictors,
therefore we only need to know the number of categories for each predictor:  `N_obs`, `N_age`, and `N_eth`.
In order generate the binomial data, we can either specify the number of tests per demographic,
or specify the total number of individual observations (`N_obs`) and then allocating percentages of
the total to each demographic.  Here we choose the latter approach.
We further allow the user to specify the global intercept `alpha` instead of using a
PRNG function to generate it; this too, can be made fully automatic.

Data is simulated in the generated quantities block as follows:

+ Create a vector of per-category coefficient values

+ Create a simplex which describes the per-category proportion of the population
in order to generate unequal number of observations across the demographic category.

+ Generate the data vectors `sex`, `age`, `eth`, `tests`, `pos_tests` using a nested loop:

```stan
  for (i_sex in 1:2) {
    for (i_age in 1:N_age) {
      for (i_eth in 1:N_eth) {
        // compute per-category data and observed outcome
      }
    }
  }
```

Putting this all together, we have the following data-generating program:

```stan
data {
  int<lower=3> N_obs, N_age, N_eth;
  real log_intercept;
}
transformed data {
  int strata = 2 * N_age * N_eth;
}
generated quantities {
  // parameter values
  real alpha = log_intercept;
  real beta_female = normal_rng(0, 2.5);
  real sigma_age = normal_rng(0, 2.5);
  vector[N_age] beta_age;
  for (n in 1:N_age) {
    beta_age[n] = normal_rng(0, sigma_age);
  }
  real sigma_eth = normal_rng(0, 2.5);
  vector[N_eth] beta_eth;
  for (n in 1:N_eth) {
    beta_eth[n] = normal_rng(0, sigma_eth);
  }
  // observations per category (unequal)
  vector[2] pct_sex = [0.4, 0.6]';
  vector[N_age] pct_age = dirichlet_rng(rep_vector(2, N_age));
  vector[N_eth] pct_eth = dirichlet_rng(rep_vector(1, N_eth));

  // data
  int N = strata
  array[N] int sex, age, eth, pos_tests, tests;
  array[N] real prob_pos_test;
  int i = 0;
  for (i_sex in 1:2) {
    for (i_age in 1:N_age) {
      for (i_eth in 1:N_eth) {
        i += 1;
        sex[i] = i_sex - 1; // coding M == 0, F == 1
        age[i] = i_age;
        eth[i] = i_eth;
        tests[i] = to_int(pct_sex[i_sex] * pct_age[i_age] * pct_eth[i_eth] * N_obs);
        prob_pos_test[i] = inv_logit(alpha + beta_female * (i_sex - 1) + beta_age[i_age] + beta_eth[i_eth]);
        pos_tests[i] = binomial_rng(tests[i], prob_pos_test[i]);
      }
    }
  }
}
```

#### Exegesis of the `generated quantities` block

To generate the true per-category parameters, we use Stan's PRNG functions.
First use the same hyperparameters as in the Stan model to
generate the hierarchical variance parameters `sigma_age` and `sigma_eth`,
then we generage the true per-category parameters accordingly.

To generate a simplex of per-category percentages we use the `dirichlet_rng` function.
The per demographic percentage is simply the product of the these percentages,
when is then multiplied by the total number of observations yielding tests per category.

To generate the data vectors, we use the nested `for` loop, where the innermost loop keeps track
of the index for the current demographic.

```{python}
hier_indata = {"N_obs":10000, "N_eth":3, "N_age":5, "log_intercept":-2.0}
hier_binom_datagen = CmdStanModel(stan_file='hier_binomial_datagen.stan')
hier_binom_datagen.sample(data=hier_indata, chains=1, iter_warmup=0, iter_sampling=100, show_progress=False)
```

## Generating Data for a Hierarchical Model with a Non-standard Likelihood

When modeling the outcome from a diagnostic test,
we need to account for test sensitivity and specificity.


### Test Sensitivity and Specificity

Test sensitivity is the probability that an individual who has the disease will test positive ("true positive").
Test specificity is the probability that an individual who doesn't have the disease will test negative ("true negative").

Given an underlying disease prevalence, we can model the probability of a positive test as the combination of the probability of a true positive and the probability of a false positive:

```stan
positive ~ bernoulli(prev * sens + (1 - prev) * (1 - spec);
```

If we aggregate the data by population stratum, this becomes a binomial:

```stan
num_positive ~ binomial(num_tests, prev * sens + (1 - prev) * (1 - spec));
```

### Putting it together

If the test is perfectly accurate - where both the sensitivity and specificity are 1,
the likelihood is the same as in the hierarchical model above:

```stan
num_positive ~ binomial(num_tests, prev);
```

Most (if not all) tests are not perfectly accurate.
The commonly used tests for Covid-19 are the nasal swabs where
either the sample is sent to a lab for PCR testing, or the at-home test kits ("lateral flow"),
which are generally estimated to have a sensitivity of 70-75% and 65%, respectively.
Both kinds of test have a specificity of close to 99.5%.
Given these estimates, we can build a model where sensitivity and specificity
are passed in as data and used in the model block to compute the likelihood.

If we assume that test sensitivity and specificity is independent of demographic factors,
we can write a Stan model which takes into account both demographic factors as well
as test sensitivity and specificity.



